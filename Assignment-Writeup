#The assingment requires to use the weight lifting exercise data set and create a classification algorithm. The algorithm 
#is to classify the manner in which the subjects perform their exercise (correct and incorrect variations of the same 
#exercise). The study subjects wore motion tracking devices in order to capture various data about their exercise. There 
#are 5 class levels (variations of the same exercise) that are required for training the algorithm and using it to 
#subsequently make predictions on 20 pre-determined out-of-sample examples.

#I downloded the data sets "pml-training.csv" and "pml-testing.csv" directly to my hard-drive and them imported them into
# my work directory in R.

#Call the necessary libraries for the analysis
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)

#Read In The files
#Set Work Directory
setwd("~/Career/Practical_Machine_Learning/Assignments")

#Set the seed number if I would like to recreate the exact results later
set.seed(1357)

#Read In The files
d_trn <- read.csv("pml-training.csv", header=TRUE,na.strings=c("NA","#DIV/0!",""))
d_tst <- read.csv("pml-testing.csv", header=TRUE,na.strings=c("NA","#DIV/0!",""))

dim(d_trn); dim(d_tst)
#[1] 19622    160
#[1]  20      160

#160 features are present in the data. The data set with 20 observation will be used to perform the final prediction. 
#The "d_trn" data will be be partitioned into training and testing 50/50. Conventionally, the split is typically 60/40,
#but I decided to reduce the processign time (random forests take a long time) on with the smaller training data set.
#The partition is doneon the classification target variable "classe."

# Partition the Training Data
inTrain <- createDataPartition(y=d_trn$classe, p=0.5, list=FALSE)
wkTraining <- d_trn[inTrain, ]
wkTesting <- d_trn[-inTrain, ]
dim(wkTraining); dim(wkTesting)

#[1] 9812  160
#[1] 9810  160

#Upon preliminary data observation, I noticed quite a few field with statistical summaries like standard deviation
#curtosis, etc that were derived from the primary exercise repetitions. Many of these fields contained "NA" values.
#These fields are not very useful to make predictiions unless the predictions are made on repetitive exercise and
#not on a one-time obsevation similar to the ones in the 20 sample data. Features with NA of more than 50% of
observations as well as near zero variance field will be removed.

#Eliminate Zero Variance Predictors

wkDataNZV <- nearZeroVar(wkTraining, saveMetrics=TRUE)
wkDataNZV

#Remove the features
wkNZVvars <- names(wkTraining) %in% c("new_window","num_window",
"kurtosis_yaw_belt",
"skewness_yaw_belt",
"amplitude_yaw_belt",
"avg_roll_arm",
"stddev_roll_arm",
"var_roll_arm",
"avg_pitch_arm",
"stddev_pitch_arm",
"var_pitch_arm",
"avg_yaw_arm",
"stddev_yaw_arm",
"var_yaw_arm",
"amplitude_roll_arm",
"kurtosis_yaw_dumbbell",
"skewness_yaw_dumbbel",
"amplitude_yaw_dumbbel",
"kurtosis_yaw_forearm",
"skewness_yaw_forearm",
"amplitude_roll_forearm",
"amplitude_yaw_forearm",
"avg_roll_forearm",
"stddev_roll_forearm",
"var_roll_forearm",
"avg_pitch_forearm",
"stddev_pitch_forearm",
"var_pitch_forearm",
"avg_yaw_forearm",
"stddev_yaw_forearm",
"var_yaw_forearm",
"min_pitch_arm",
"skewness_yaw_dumbbell",
"amplitude_yaw_dumbbell",
#remove certain features
"X",
"raw_timestamp_part_1",
"raw_timestamp_part_2",
"cvtd_timestamp",
"user_name"
)

wkTraining <- wkTraining[!wkNZVvars]
dim(wkTraining)

#Eliminate Predictors where NA >= 51%
hldTraining <- wkTraining

for(i in 1:length(wkTraining)) 
	{ 
        if(sum( is.na(wkTraining[, i] ) ) /nrow(wkTraining) >= .51 ) #NAs > 51% of total Ns
		{
		        for(j in 1:length(hldTraining))
				 {
            			if(length( grep(names(wkTraining[i]), names(hldTraining)[j]) ) ==1)  #if same name
		{
                hldTraining <- hldTraining[ , -j] #Remove that variable
            	}   
        			} 
    	}
}

dim(hldTraining)
#[1] 9812   53

rm(wkTraining)
wkTraining <- hldTraining
rm(hldTraining)

#Only 53 features remain in the data after irrelevant information was removed.

#As an experiment, I will bootstrap my training data in an effort to cross validate the quality of some algorithms
#I will be bulding. This is a good check for how the error will behave under an out-of-sample scenario. 

#BOOT STRAPPING Cross Validation Object
fitControl <- trainControl(
                           method = "repeatedcv",
                           number = 4,
                           repeats = 4)

Machine Learning Part: I will train 3 algorithms - 
	#Decision Tree
	#Random Forests
	#Boosting

#Descriptive statistics revealed that 4 predictors were highly correlated (>.9) with some of the other predictors.
#I will eliminate those because they do not carry any additional information signal
#Removing highly correlated features
wkTraining <- subset(wkTraining,select=-c(gyros_belt_x,gyros_belt_y,magnet_dumbbell_x,magnet_dumbbell_y))

#DECISION TREE (DT)
mod_DT <- train(wkTraining$classe ~ ., method="rpart",data=wkTraining)

print(mod_DT)
CART 

9812 samples
  48 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 9812, 9812, 9812, 9812, 9812, 9812, ... 
Resampling results across tuning parameters:

  cp          Accuracy   Kappa       Accuracy SD  Kappa SD  
  0.02242951  0.6067790  0.49993101  0.04281592   0.06093116
  0.02734264  0.5613997  0.43881968  0.06190937   0.09270286
  0.11777271  0.3343041  0.07564283  0.03972848   0.06310175

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was cp = 0.02242951. 
print(mod_DT$finalModel)
n= 9812 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

   1) root 9812 7022 A (0.28 0.19 0.17 0.16 0.18)  
     2) roll_belt< 130.5 8973 6189 A (0.31 0.21 0.19 0.18 0.11)  
       4) pitch_forearm< -34.15 798    3 A (1 0.0038 0 0 0) *
       5) pitch_forearm>=-34.15 8175 6186 A (0.24 0.23 0.21 0.2 0.12)  
        10) yaw_belt>=169.5 385   37 A (0.9 0.039 0 0.047 0.01) *
        11) yaw_belt< 169.5 7790 5909 B (0.21 0.24 0.22 0.2 0.12)  
          22) magnet_dumbbell_z< -93.5 895  349 A (0.61 0.28 0.045 0.048 0.015) *
          23) magnet_dumbbell_z>=-93.5 6895 5224 C (0.16 0.24 0.24 0.22 0.14)  
            46) pitch_belt< -42.95 399   67 B (0.018 0.83 0.095 0.028 0.028) *
            47) pitch_belt>=-42.95 6496 4863 C (0.17 0.2 0.25 0.24 0.15)  
              94) roll_belt< 117.5 4305 3122 B (0.16 0.27 0.19 0.18 0.19)  
               188) accel_dumbbell_z< 32.5 3209 2394 C (0.21 0.21 0.25 0.2 0.13)  
                 376) magnet_dumbbell_z< -14.5 805  465 A (0.42 0.35 0.0012 0.13 0.098) *
                 377) magnet_dumbbell_z>=-14.5 2404 1590 C (0.13 0.16 0.34 0.22 0.15)  
                   754) magnet_dumbbell_z>=284.5 320   86 A (0.73 0.12 0.072 0.069 0.0062) *
                   755) magnet_dumbbell_z< 284.5 2084 1293 C (0.04 0.16 0.38 0.25 0.17)  
                    1510) accel_forearm_x>=-99.5 1480  847 C (0.045 0.22 0.43 0.11 0.2) *
                    1511) accel_forearm_x< -99.5 604  253 D (0.03 0.03 0.26 0.58 0.098) *
               189) accel_dumbbell_z>=32.5 1096  574 B (0.017 0.48 0.0027 0.14 0.37) *
              95) roll_belt>=117.5 2191 1376 C (0.19 0.052 0.37 0.34 0.05)  
               190) roll_dumbbell< 59.31113 1735  952 C (0.23 0.056 0.45 0.22 0.048) *
               191) roll_dumbbell>=59.31113 456   88 D (0.031 0.033 0.07 0.81 0.059) *
     3) roll_belt>=130.5 839    6 E (0.0072 0 0 0 0.99) *
     
     #Visualize the trained decision tree
     fancyRpartPlot(mod_DT$finalModel)
     {insert the image here}
     
     #Assess performance on the testing data
     prd_DT <- predict(mod_DT, newdata=wkTesting)
> DT_CMX <- confusionMatrix(prd_DT, wkTesting$classe)
> DT_CMX
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2248  635   83  211   92
         B   25  808   42  150  455
         C  467  409 1385  504  365
         D   42   46  201  743   93
         E    8    0    0    0  798

Overall Statistics
                                          
               Accuracy : 0.6098          
                 95% CI : (0.6001, 0.6195)
    No Information Rate : 0.2844          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5039          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.8057  0.42571   0.8095  0.46206  0.44260
Specificity            0.8546  0.91507   0.7845  0.95343  0.99900
Pos Pred Value         0.6877  0.54595   0.4425  0.66044  0.99007
Neg Pred Value         0.9171  0.86915   0.9512  0.90040  0.88838
Prevalence             0.2844  0.19348   0.1744  0.16391  0.18379
Detection Rate         0.2292  0.08236   0.1412  0.07574  0.08135
Detection Prevalence   0.3332  0.15087   0.3191  0.11468  0.08216
Balanced Accuracy      0.8301  0.67039   0.7970  0.70775  0.72080
     
     
     
     
