<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Practical Machine Learning Project by amak222</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Practical Machine Learning Project</h1>
        <h2>My machine learning (ML) class work</h2>
        <a href="https://github.com/amak222/Practical-ML-Class" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h3>
<a id="project-objective" class="anchor" href="#project-objective" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Objective</h3>

<p>The assignment requires to use the weight lifting exercise data set and create a classification algorithm. The algorithm 
is to classify the manner in which the subjects perform their exercise (correct and incorrect variations of the same 
exercise). The study subjects wore motion tracking devices in order to capture various data about their exercise. There 
are 5 class levels (variations of the same exercise) that are required for training the algorithm and using it to 
subsequently make predictions on 20 predetermined out-of-sample examples.</p>

<h3>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data</h3>

<p>I downloaded the data sets "pml-training.csv" and "pml-testing.csv" directly to my hard-drive and then imported them into
my work directory in R.</p>

<p>Call the necessary libraries that will be used in the data exploration and algorithm training later.</p>

<pre><code>library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rattle)
</code></pre>

<h3>
<a id="reading-the-files-into-the-work-directory" class="anchor" href="#reading-the-files-into-the-work-directory" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reading the Files Into the Work Directory</h3>

<p>Set Work Directory</p>

<pre><code>setwd("~/Career/Practical_Machine_Learning/Assignments")
</code></pre>

<p>Set the seed number if I would like to recreate the exact results later.</p>

<pre><code>set.seed(1357)
</code></pre>

<p>Read In The files</p>

<pre><code>d_trn &lt;- read.csv("pml-training.csv", header=TRUE,na.strings=c("NA","#DIV/0!",""))
d_tst &lt;- read.csv("pml-testing.csv", header=TRUE,na.strings=c("NA","#DIV/0!",""))
dim(d_trn); dim(d_tst)
</code></pre>

<p>Rows and columns</p>

<p>[1] 19622    160</p>

<p>[1]  20      160</p>

<p>160 features are present in the data. The data set with 20 observation will be used to perform the final prediction. 
The "d_trn" data will be be partitioned into training and testing 50/50. Conventionally, the split is typically 60/40,
but I decided to reduce the processing time (random forests take a long time) with the smaller training data set.
The partition is done on the classification target variable "classe."</p>

<h3>
<a id="data-partition" class="anchor" href="#data-partition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Partition</h3>

<pre><code>inTrain &lt;- createDataPartition(y=d_trn$classe, p=0.5, list=FALSE)
wkTraining &lt;- d_trn[inTrain, ]
wkTesting &lt;- d_trn[-inTrain, ]
dim(wkTraining); dim(wkTesting)
</code></pre>

<p>[1] 9812  160</p>

<p>[1] 9810  160</p>

<p>Upon preliminary data observation, I noticed quite a few field with statistical summaries like standard deviation
curtosis, etc that were derived from the primary exercise repetitions. Many of these fields contained "NA" values.
These fields are not very useful to make predictions unless the predictions are made on repetitive exercise and
not on a one-time observation similar to the ones in the 20 sample data. Features with NA of more than 50% of
observations as well as near zero variance field will be removed.</p>

<p>Among other irrelevant features were the features representing user names, date and time stamps and some indication what
observation window the measurements fell into. These predictors are highly correlated with the outcome variable because
that's how the experiment was designed and conducted. I will remove them accordingly.</p>

<h3>
<a id="near-zero-variance-predictors-nzv" class="anchor" href="#near-zero-variance-predictors-nzv" aria-hidden="true"><span class="octicon octicon-link"></span></a>Near Zero Variance Predictors (NZV)</h3>

<p>Let's determine what predictors have no or little variability in them</p>

<p>wkDataNZV &lt;- nearZeroVar(wkTraining, saveMetrics=TRUE)
   wkDataNZV</p>

<p>Remove the NZV and irrelevant predictors from the data</p>

<pre><code>wkNZVvars &lt;- names(wkTraining) %in%  c("new_window","num_window","kurtosis_yaw_belt","skewness_yaw_belt",
"amplitude_yaw_belt","avg_roll_arm","stddev_roll_arm","var_roll_arm","avg_pitch_arm","stddev_pitch_arm","var_pitch_arm",
"avg_yaw_arm","stddev_yaw_arm","var_yaw_arm","amplitude_roll_arm","kurtosis_yaw_dumbbell","skewness_yaw_dumbbel",
"amplitude_yaw_dumbbel","kurtosis_yaw_forearm","skewness_yaw_forearm","amplitude_roll_forearm","amplitude_yaw_forearm",
"avg_roll_forearm","stddev_roll_forearm","var_roll_forearm","avg_pitch_forearm","stddev_pitch_forearm",
"var_pitch_forearm","avg_yaw_forearm","stddev_yaw_forearm","var_yaw_forearm","min_pitch_arm","skewness_yaw_dumbbell",
"amplitude_yaw_dumbbell",
#irrelevant features
"X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","user_name")
</code></pre>

<p>wkTraining &lt;- wkTraining[!wkNZVvars]</p>

<p>I will also eliminate predictors where NA &gt;= 51%</p>

<pre><code>hldTraining &lt;- wkTraining
for(i in 1:length(wkTraining)) 
{ if(sum( is.na(wkTraining[, i] ) ) /nrow(wkTraining) &gt;= .51 ) #NAs &gt; 51% of total Ns
{for(j in 1:length(hldTraining))
{if(length( grep(names(wkTraining[i]), names(hldTraining)[j]) ) ==1)  #if same name
{hldTraining &lt;- hldTraining[ , -j] #Remove that variable}   
 }      }}

dim(hldTraining)
</code></pre>

<p>[1] 9812   53</p>

<pre><code>rm(wkTraining)
wkTraining &lt;- hldTraining
rm(hldTraining)
</code></pre>

<p>Only 53 features remain in the data after irrelevant predictors have been removed.</p>

<h3>
<a id="bootstrapping-of-the-training-data" class="anchor" href="#bootstrapping-of-the-training-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bootstrapping of the Training Data</h3>

<p>As an experiment, I decided to bootstrap my training data in an effort to cross validate the quality of some algorithms
I will be building. This is a good check for how the error will behave under an out-of-sample scenario. </p>

<pre><code>fitControl &lt;- trainControl(
                       method = "repeatedcv",
                       number = 4,
                       repeats = 4)
</code></pre>

<p>The fitControl object will be later applied to a boosting algorithm.</p>

<h2>
<a id="machine-learning-algorithms" class="anchor" href="#machine-learning-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning Algorithms</h2>

<p>I will train three algorithms - </p>

<ol>
<li><p>Decision Tree</p></li>
<li><p>Random Forests</p></li>
<li><p>Boosting</p></li>
</ol>

<p>Before I proceeded, I ran a descriptive statistics analysis. The analysis revealed that 4 predictors were highly correlated (&gt;.9) with some of the other predictors. I will eliminate those because they do not carry any additional information signal.</p>

<p>Removing highly correlated features</p>

<pre><code>wkTraining &lt;- subset(wkTraining,select=-c(gyros_belt_x,gyros_belt_y,magnet_dumbbell_x,magnet_dumbbell_y))
</code></pre>

<h3>
<a id="1-decision-tree-dt" class="anchor" href="#1-decision-tree-dt" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. DECISION TREE (DT)</h3>

<pre><code>mod_DT &lt;- train(wkTraining$classe ~ ., method="rpart",data=wkTraining)
print(mod_DT)
</code></pre>

<hr>

<p>CART 
9812 samples
48 predictor
5 classes: 'A', 'B', 'C', 'D', 'E' </p>

<hr>

<p>No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 9812, 9812, 9812, 9812, 9812, 9812, ... 
Resampling results across tuning parameters:
 cp          Accuracy   Kappa       Accuracy SD  Kappa SD<br>
0.02242951  0.6067790  0.49993101  0.04281592   0.06093116
0.02734264  0.5613997  0.43881968  0.06190937   0.09270286
0.11777271  0.3343041  0.07564283  0.03972848   0.06310175</p>

<p>Accuracy was used to select the optimal model using  the largest value. The final value used for the model was cp =0.02242951. 
n= 9812</p>

<p>Visualize the trained decision tree</p>

<pre><code>fancyRpartPlot(mod_DT$finalModel)
</code></pre>

<p>{insert the image here}</p>

<p>Assess the performance on the testing data</p>

<pre><code>prd_DT &lt;- predict(mod_DT, newdata=wkTesting)
DT_CMX &lt;- confusionMatrix(prd_DT, wkTesting$classe)
DT_CMX
</code></pre>

<p>Confusion Matrix and Statistics</p>

<pre><code>      Reference
</code></pre>

<p>Prediction    A    B    C    D    E
         A 2248  635   83  211   92
         B   25  808   42  150  455
         C  467  409 1385  504  365
         D   42   46  201  743   93
         E    8    0    0    0  798</p>

<p>Overall Statistics</p>

<pre><code>          Accuracy : 0.6098          
            95% CI : (0.6001, 0.6195)
No Information Rate : 0.2844          
P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
              Kappa : 0.5039          
</code></pre>

<p>Mcnemar's Test P-Value : &lt; 2.2e-16       </p>

<p>Statistics by Class:</p>

<pre><code>                 Class: A Class: B Class: C Class: D Class: E
</code></pre>

<p>Sensitivity            0.8057  0.42571   0.8095  0.46206  0.44260
Specificity            0.8546  0.91507   0.7845  0.95343  0.99900
Pos Pred Value         0.6877  0.54595   0.4425  0.66044  0.99007
Neg Pred Value         0.9171  0.86915   0.9512  0.90040  0.88838
Prevalence             0.2844  0.19348   0.1744  0.16391  0.18379
Detection Rate         0.2292  0.08236   0.1412  0.07574  0.08135
Detection Prevalence   0.3332  0.15087   0.3191  0.11468  0.08216
Balanced Accuracy      0.8301  0.67039   0.7970  0.70775  0.72080</p>

<p>Achieved accuracy is .61 in testing and the confusion matrix shows that many cases are misclassified. Possibly a different approach is necessary.</p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/amak222/Practical-ML-Class/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/amak222/Practical-ML-Class/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/amak222/Practical-ML-Class"></a> is maintained by <a href="https://github.com/amak222">amak222</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
