{"name":"Practical Machine Learning Project","tagline":"My machine learning (ML) class work","body":"### Project Objective\r\nThe assignment requires to use the weight lifting exercise data set and create a classification algorithm. The algorithm \r\nis to classify the manner in which the subjects perform their exercise (correct and incorrect variations of the same \r\nexercise). The study subjects wore motion tracking devices in order to capture various data about their exercise. There \r\nare 5 class levels (variations of the same exercise) that are required for training the algorithm and using it to \r\nsubsequently make predictions on 20 predetermined out-of-sample examples.\r\n\r\n### Data\r\nI downloaded the data sets \"pml-training.csv\" and \"pml-testing.csv\" directly to my hard-drive and then imported them into\r\nmy work directory in R.\r\n\r\nCall the necessary libraries that will be used in the data exploration and algorithm training later.\r\n\r\n    library(lattice)\r\n    library(ggplot2)\r\n    library(caret)\r\n    library(rpart)\r\n    library(rattle)\r\n\r\n### Reading the Files Into the Work Directory\r\n\r\nSet Work Directory\r\n\r\n    setwd(\"~/Career/Practical_Machine_Learning/Assignments\")\r\n\r\nSet the seed number if I would like to recreate the exact results later.\r\n\r\n    set.seed(1357)\r\n\r\nRead In The files\r\n\r\n    d_trn <- read.csv(\"pml-training.csv\", header=TRUE,na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\r\n    d_tst <- read.csv(\"pml-testing.csv\", header=TRUE,na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\r\n    dim(d_trn); dim(d_tst)\r\n\r\nRows and columns\r\n\r\n[1] 19622    160\r\n\r\n[1]  20      160\r\n\r\n160 features are present in the data. The data set with 20 observation will be used to perform the final prediction. \r\nThe \"d_trn\" data will be be partitioned into training and testing 50/50. Conventionally, the split is typically 60/40,\r\nbut I decided to reduce the processing time (random forests take a long time) with the smaller training data set.\r\nThe partition is done on the classification target variable \"classe.\"\r\n\r\n### Data Partition\r\n\r\n    inTrain <- createDataPartition(y=d_trn$classe, p=0.5, list=FALSE)\r\n    wkTraining <- d_trn[inTrain, ]\r\n    wkTesting <- d_trn[-inTrain, ]\r\n    dim(wkTraining); dim(wkTesting)\r\n\r\n[1] 9812  160\r\n\r\n[1] 9810  160\r\n\r\nUpon preliminary data observation, I noticed quite a few field with statistical summaries like standard deviation\r\ncurtosis, etc that were derived from the primary exercise repetitions. Many of these fields contained \"NA\" values.\r\nThese fields are not very useful to make predictions unless the predictions are made on repetitive exercise and\r\nnot on a one-time observation similar to the ones in the 20 sample data. Features with NA of more than 50% of\r\nobservations as well as near zero variance field will be removed.\r\n\r\nAmong other irrelevant features were the features representing user names, date and time stamps and some indication what\r\nobservation window the measurements fell into. These predictors are highly correlated with the outcome variable because\r\nthat's how the experiment was designed and conducted. I will remove them accordingly.\r\n\r\n### Near Zero Variance Predictors (NZV)\r\n\r\nLet's determine what predictors have no or little variability in them\r\n\r\n   wkDataNZV <- nearZeroVar(wkTraining, saveMetrics=TRUE)\r\n   wkDataNZV\r\n\r\nRemove the NZV and irrelevant predictors from the data\r\n\r\n    wkNZVvars <- names(wkTraining) %in%  c(\"new_window\",\"num_window\",\"kurtosis_yaw_belt\",\"skewness_yaw_belt\",\r\n    \"amplitude_yaw_belt\",\"avg_roll_arm\",\"stddev_roll_arm\",\"var_roll_arm\",\"avg_pitch_arm\",\"stddev_pitch_arm\",\"var_pitch_arm\",\r\n    \"avg_yaw_arm\",\"stddev_yaw_arm\",\"var_yaw_arm\",\"amplitude_roll_arm\",\"kurtosis_yaw_dumbbell\",\"skewness_yaw_dumbbel\",\r\n    \"amplitude_yaw_dumbbel\",\"kurtosis_yaw_forearm\",\"skewness_yaw_forearm\",\"amplitude_roll_forearm\",\"amplitude_yaw_forearm\",\r\n    \"avg_roll_forearm\",\"stddev_roll_forearm\",\"var_roll_forearm\",\"avg_pitch_forearm\",\"stddev_pitch_forearm\",\r\n    \"var_pitch_forearm\",\"avg_yaw_forearm\",\"stddev_yaw_forearm\",\"var_yaw_forearm\",\"min_pitch_arm\",\"skewness_yaw_dumbbell\",\r\n    \"amplitude_yaw_dumbbell\",\r\n    #irrelevant features\r\n    \"X\",\"raw_timestamp_part_1\",\"raw_timestamp_part_2\",\"cvtd_timestamp\",\"user_name\")\r\n   wkTraining <- wkTraining[!wkNZVvars]\r\n\r\nI will also eliminate predictors where NA >= 51%\r\n\r\n    hldTraining <- wkTraining\r\n    for(i in 1:length(wkTraining)) \r\n    { if(sum( is.na(wkTraining[, i] ) ) /nrow(wkTraining) >= .51 ) #NAs > 51% of total Ns\r\n    {for(j in 1:length(hldTraining))\r\n    {if(length( grep(names(wkTraining[i]), names(hldTraining)[j]) ) ==1)  #if same name\r\n    {hldTraining <- hldTraining[ , -j] #Remove that variable}   \r\n     }     \t}}\r\n\r\n    dim(hldTraining)\r\n\r\n[1] 9812   53\r\n\r\n    rm(wkTraining)\r\n    wkTraining <- hldTraining\r\n    rm(hldTraining)\r\n\r\nOnly 53 features remain in the data after irrelevant predictors have been removed.\r\n\r\n### Bootstrapping of the Training Data\r\nAs an experiment, I decided to bootstrap my training data in an effort to cross validate the quality of some algorithms\r\nI will be building. This is a good check for how the error will behave under an out-of-sample scenario. \r\n\r\n    fitControl <- trainControl(\r\n                           method = \"repeatedcv\",\r\n                           number = 4,\r\n                           repeats = 4)\r\n\r\nThe fitControl object will be later applied to a boosting algorithm.\r\n\r\n## Machine Learning Algorithms\r\n\r\nI will train three algorithms - \r\n\r\n1. Decision Tree\r\n\r\n2. Random Forests\r\n\r\n3. Boosting\r\n\r\nBefore I proceeded, I ran a descriptive statistics analysis. The analysis revealed that 4 predictors were highly correlated (>.9) with some of the other predictors. I will eliminate those because they do not carry any additional information signal.\r\n\r\nRemoving highly correlated features\r\n\r\n    wkTraining <- subset(wkTraining,select=-c(gyros_belt_x,gyros_belt_y,magnet_dumbbell_x,magnet_dumbbell_y))\r\n\r\n\r\n###1. DECISION TREE (DT)\r\n\r\n    mod_DT <- train(wkTraining$classe ~ ., method=\"rpart\",data=wkTraining)\r\n    print(mod_DT)\r\n\r\n    CART \r\n    9812 samples\r\n    48 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\n    No pre-processing\r\n    Resampling: Bootstrapped (25 reps) \r\n    Summary of sample sizes: 9812, 9812, 9812, 9812, 9812, 9812, ... \r\n\r\n    Resampling results across tuning parameters:\r\n     cp          Accuracy   Kappa       Accuracy SD  Kappa SD  \r\n    0.02242951  0.6067790  0.49993101  0.04281592   0.06093116\r\n    0.02734264  0.5613997  0.43881968  0.06190937   0.09270286\r\n    0.11777271  0.3343041  0.07564283  0.03972848   0.06310175\r\n\r\n    Accuracy was used to select the optimal model using  the largest value. The final value used for the model was cp =0.02242951. \r\n\r\n    n= 9812\r\n\r\nVisualize the trained decision tree\r\n\r\n    fancyRpartPlot(mod_DT$finalModel)\r\n\r\n{insert the image here}\r\n     \r\nAssess the performance on the testing data\r\n\r\n    prd_DT <- predict(mod_DT, newdata=wkTesting)\r\n    DT_CMX <- confusionMatrix(prd_DT, wkTesting$classe)\r\n    DT_CMX\r\n\r\nConfusion Matrix and Statistics\r\n\r\n    Reference\r\n    Prediction    A    B    C    D    E\r\n         A 2248  635   83  211   92\r\n         B   25  808   42  150  455\r\n         C  467  409 1385  504  365\r\n         D   42   46  201  743   93\r\n         E    8    0    0    0  798\r\n\r\nOverall Statistics\r\n                                          \r\n     Accuracy : 0.6098          \r\n     95% CI : (0.6001, 0.6195)\r\n    No Information Rate : 0.2844          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                  Kappa : 0.5039          \r\n     Mcnemar's Test P-Value : < 2.2e-16       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.8057  0.42571   0.8095  0.46206  0.44260\r\n    Specificity            0.8546  0.91507   0.7845  0.95343  0.99900\r\n    Pos Pred Value         0.6877  0.54595   0.4425  0.66044  0.99007\r\n    Neg Pred Value         0.9171  0.86915   0.9512  0.90040  0.88838\r\n    Prevalence             0.2844  0.19348   0.1744  0.16391  0.18379\r\n    Detection Rate         0.2292  0.08236   0.1412  0.07574  0.08135\r\n    Detection Prevalence   0.3332  0.15087   0.3191  0.11468  0.08216\r\n    Balanced Accuracy      0.8301  0.67039   0.7970  0.70775  0.72080\r\n     \r\nAchieved accuracy is .61 in testing and the confusion matrix shows that many cases are misclassified. Possibly a different approach is necessary.\r\n\r\n###2. RANDOM FORESTS (RF)\r\n\r\n    mod_RF <- train(classe ~. , method=\"rf\", data=wkTraining, prox=TRUE)\r\n    prd_RF <- predict(mod_RF, wkTesting)\r\n    RF_CMX <- confusionMatrix(prd_RF, wkTesting$classe)\r\n    RF_CMX\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\n    Prediction    A    B    C    D    E\r\n            A 2778   28    0    0    0\r\n            B   11 1854   16    3    5\r\n            C    1   13 1688   14    7\r\n            D    0    2    7 1585    5\r\n            E    0    1    0    6 1786\r\n\r\nOverall Statistics\r\n                                          \r\n         Accuracy : 0.9879          \r\n         95% CI : (0.9855, 0.9899)\r\n    No Information Rate : 0.2844          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9847          \r\n     Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9957   0.9768   0.9866   0.9857   0.9906\r\n    Specificity            0.9960   0.9956   0.9957   0.9983   0.9991\r\n    Pos Pred Value         0.9900   0.9815   0.9797   0.9912   0.9961\r\n    Neg Pred Value         0.9983   0.9944   0.9972   0.9972   0.9979\r\n    Prevalence             0.2844   0.1935   0.1744   0.1639   0.1838\r\n    Detection Rate         0.2832   0.1890   0.1721   0.1616   0.1821\r\n    Detection Prevalence   0.2860   0.1926   0.1756   0.1630   0.1828\r\n   Balanced Accuracy      0.9959   0.9862   0.9911   0.9920   0.9948\r\n\r\nThe RF algorithm took a while to run, but the results look promising on the testing data. Achieved accuracy = .988 and somewhat a small number of cases were misclasified as shown by the confusion matrix\r\n\r\n\r\n###3. GRADIENT BOOSTING MODEL WITH BOOT-STRAPPING\r\n\r\n    mod_B_cv <- train(wkTraining$classe ~ ., data=wkTraining, method=\"gbm\", trControl = fitControl, verbose=FALSE)\r\n \r\n    prd_B_cv <- predict(mod_B_cv, wkTesting)\r\n    B_CMX_cv <- confusionMatrix(prd_B_cv, wkTesting$classe)\r\n    B_CMX_cv\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\n    Prediction    A    B    C    D    E\r\n             A 2719   85    4    2   10\r\n             B   49 1750   52    6   28\r\n             C   16   57 1636   38   17\r\n             D    3    3   14 1554   30\r\n             E    3    3    5    8 1718\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9559          \r\n                 95% CI : (0.9516, 0.9598)\r\n      No Information Rate : 0.2844          \r\n      P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9442          \r\n     Mcnemar's Test P-Value : 1.398e-11       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\n    Sensitivity            0.9746   0.9220   0.9562   0.9664   0.9529\r\n    Specificity            0.9856   0.9829   0.9842   0.9939   0.9976\r\n    Pos Pred Value         0.9642   0.9284   0.9274   0.9688   0.9891\r\n    Neg Pred Value         0.9898   0.9813   0.9907   0.9934   0.9895\r\n    Prevalence             0.2844   0.1935   0.1744   0.1639   0.1838\r\n    Detection Rate         0.2772   0.1784   0.1668   0.1584   0.1751\r\n    Detection Prevalence   0.2875   0.1922   0.1798   0.1635   0.1771\r\n    Balanced Accuracy      0.9801   0.9525   0.9702   0.9802   0.9752\r\n\r\nThe boosting algorithm performed well, but it was not as accurate as the random forests. The acheived accuracy was 0.96. I also ran a boosting algorithm, but without the boot strapping component and the accuracy went down to 0.95. The boot strapping impact was rather small.\r\n\r\nAfter comparing the predictive accuracy for each 4 algorithms I ran, I selected the highest accuracy with the RF algorithm. I feel comfortable using accuracy in this case because our outcome classes were not rare events.\r\n\r\nFinally, I used the RF and Boosting algorithms to produce prediction on the 20 held out examples. They both produced identical predictions below.\r\n\r\n    prd_1 <- predict(mod_RF, newdata = d_tst)\r\n    prd_2 <- predict(mod_B_cv, newdata = d_tst)\r\n    prd_1; prd_2\r\n    [1] B A B A A E D B A A B C B A E E A B B B\r\n\r\n    [1] B A B A A E D B A A B C B A E E A B B B\r\n\r\n    Levels: A B C D E\r\n\r\nJeff Leek provides a sample code that creates text files with predictions. The individual text files are uploaded to the Coursera class web site.\r\n\r\n\r\n### Writing the answers into a text file\r\n\r\n    pml_write_files = function(x){\r\n      n = length(x)\r\n      for(i in 1:n){\r\n     filename = paste0(\"problem_id_\",i,\".txt\")\r\n      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n        }\r\n     }\r\n\r\n    pml_write_files(prd_1) #predictions from the random forest algorithm\r\n\r\n##END\r\n     \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}